2024/12/6
pip install requests beautifulsoup4			#done
pip install pandas openpyxl				#done
winget install Python.Python.3.11			#done
python/PyCharm/libraries download			#done
check concepts log
check bolt.new		#consume token in GitHub	#done

2024/12/7
开始网页抓取
	确认是否为java动态加载数据	#判定为动态加载
		F12进入网站开发者工具/刷新后选择Fetch\XHR/查找成绩相关xhr（右键点击看Response）！！！！
		chrome设置禁用JS后无法加载页面信息
	成绩API	      			#API请求失败 （身份验证 或 跨域问题？）！！！！
	测试抓取
		pip install selenium			#done
		download WebDriver			#done

2024/12/8------------------测试抓取
		#成功通过selenium启动chromeDriver

2024/12/10------------------seneca网站元素抓取
	id="agree_button"(terms of use)/"bottom_Submit"(login)/"i0116"

2024/12/11------------------seneca网站元素抓取
	selenuim中密码隐私保护，自动填充失效？？？？？
	
2024/12/12------------------seneca网站元素抓取
Action chain？？？
//*[@id="main-content-inner"]    scroll目标/目标内容懒加载（点击目标分页链接后，需要time.sleep等待加载新inner再赋值）

2024/12/13------------------seneca网站元素抓取
TensorFlow ？？？
stale element error解决方法：重复给元素赋值！！！！

2024/12/16------------------成绩抓取原始代码V0
提取策略
使用 Selenium 的 find_elements_by_* 方法获取多个元素。 由于 Item Name 和 Grade 是成对出现的，我们可以先定位到包含这两类信息的父级元素，然后通过相对路径定位到具体的子元素。
================
Gemini代码示例
from selenium import webdriver
from selenium.webdriver.common.by import By
import pandas as pd

# 初始化 WebDriver
driver = webdriver.Chrome()  # 替换为你的浏览器驱动
driver.get("https://your_target_url")  # 替换为你的目标网址

# 定位到包含 Item Name 和 Grade 的父级元素 (根据实际情况调整选择器)
items = driver.find_elements(By.CSS_SELECTOR, ".row.js-rowchild-is-invokable")

# 创建一个空列表，用于存储提取的数据
data = []

# 遍历每个 item 元素，提取 Item Name 和 Grade
for item in items:
    item_name = item.find_element(By.CSS_SELECTOR, ".item-name").text
    grade = item.find_element(By.CSS_SELECTOR, ".grade").text
    data.append([item_name, grade])

# 创建 DataFrame 并保存到 Excel
df = pd.DataFrame(data, columns=["Item Name", "Grade"])
df.to_excel("grades.xlsx", index=False)

# 关闭浏览器
driver.quit()
================
注意事项
选择器准确性： 确保 CSS 选择器或 XPath 表达式能够准确地定位到目标元素。如果网页结构发生变化，可能需要调整选择器。
动态加载内容： 如果页面内容是动态加载的，可能需要等待元素加载完成再进行操作，可以使用 WebDriverWait 和 expected_conditions 来实现。
异常处理： 为了提高代码的健壮性，可以添加异常处理机制，例如处理元素未找到、网络错误等情况。
数据清洗： 提取的数据可能需要进行清洗，例如去除多余的空格、换行符等。
扩展
保存为文本文件： 可以使用 Python 的内置库 csv 或第三方库 pandas 将数据保存为 CSV 格式的文本文件。
多页面处理： 如果需要提取多个页面的数据，可以结合分页机制，逐页提取数据。
数据分析： 提取的数据可以进一步进行数据分析，例如计算平均分、最高分、最低分等。
注意： 上述代码只是一个示例，具体的实现细节需要根据你所面对的网页结构和数据格式进行调整。

问题解答
选择器如何写？ 可以提供更详细的 HTML 结构，我帮你分析并给出建议。
遇到动态加载怎么办？ 可以介绍 Selenium 中的等待机制。
数据清洗有什么技巧？ 可以提供一些常用的数据清洗方法
====================
代码优化v1.0

2024/12/17------------------代码优化v1.1
final frame:
函数get courses 		选择器调试
函数get grade 		待优化
main进入 grades分页面 	待优化
